{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer 2019 CX4240 Homework 1\n",
    "\n",
    "## Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: May 30, Thursday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "In this assignment, we only have writing questions: you are asked to answer them in the markdown cells.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "\n",
    "- Typing with Latex is highly recommended. An image scan copy of handwritten also works. If you hand write, try to be clear as much as possible. No credit may be given to unreadable handwriting.\n",
    "    \n",
    "- If you want to add any picture to your answer, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Eigenvalues and Eigenvectors for Bivariate Gaussian Distribution (25pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let two variables $X_1$ and $X_2$ are bivariately normally distributed with mean vector components $\\mu_1$ and $\\mu_2$ and co-variance matrix $\\Sigma$ shown below:\n",
    "   $$\\Sigma = \n",
    "   \\begin{bmatrix} \n",
    "    1 & r \\\\ \n",
    "    r & 1 \n",
    "    \\end{bmatrix}$$ \n",
    "\n",
    "- What is the probability distribution function of joint Gaussian $P(X_1, X_2)$? (show it with $\\mu$ and $\\Sigma$) [5pts]\n",
    "\n",
    "\n",
    "**Response**\n",
    "\n",
    "General probability distribution function for multivariate gaussian distibution is\n",
    "$$\n",
    "p(x|\\mu,\\Sigma) = {\\frac{1}{(2\\pi)^\\frac{n}{2}|\\Sigma|^\\frac{1}{2}}\\exp{{\\frac{-1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}}}\n",
    "$$\n",
    "where x is n variate random variable, $\\mu$ is $E(X)$, and $\\Sigma$ is $cov(X,X)$ or $Var(X)$\n",
    "\n",
    "Thus, probability distribution function of a bivariate random variable (n=2) is\n",
    "$$\n",
    "p(x|\\mu,\\Sigma) = {\\frac{1}{(2\\pi)|\\Sigma|}\\exp{{\\frac{-1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}}}\n",
    "$$\n",
    "\n",
    "Joint probability density function of the two bivariate random variables $X_1$ and $X_2$ is the probability density function of vector $[X_1, X_2]$, represented as follows:\n",
    "$$\n",
    "p(x_1,x_2|\\mu,\\Sigma) = {\\frac{1}{(2\\pi)|\\Sigma|}\\exp{{\\frac{-1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}}}\n",
    "$$\n",
    "where, $\\mu =\\begin{bmatrix} \n",
    "       \\mu_1\\\\\n",
    "       \\mu_2\n",
    "       \\end{bmatrix}$ and $\\Sigma = \\begin{bmatrix}\n",
    "                          1&r \\\\\n",
    "                          r&1\n",
    "                          \\end{bmatrix}$, $cov(X_1,X_2)$\n",
    "\n",
    "\n",
    "- What are the eigenvalues of co-variance matrix $\\Sigma$? [10pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "Let A be the covariance matrix.\n",
    "$A = \\begin{bmatrix}\n",
    "    1&r \\\\\n",
    "    r&1\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "Following equation holds true for eigenvalue and eigenvector matrix of A\n",
    "\n",
    "$Ax-\\lambda{x}=0$, $x\\neq{0}$\n",
    "\n",
    "$A-\\lambda{I}=0$, $x\\neq{0}$\n",
    "\n",
    "Above equality will hold true only if matrix $A-\\lambda{I}$ does not have an inverse. That is, it is a singular matrix. \n",
    "Thus, $|A - \\lambda{I}| = 0$\n",
    "\n",
    "Eigenvalues of A are the roots of equation $|A - \\lambda{I}|= 0$\n",
    "\n",
    "Solving for eigenvalues\n",
    "$\\begin{vmatrix}\n",
    "    1-\\lambda & r \\\\\n",
    "    r & \\lambda-1\n",
    "    \\end{vmatrix}$ = 0\n",
    "    \n",
    "$(1-\\lambda)^2 - r^2 = 0$\n",
    "\n",
    "$(1-\\lambda+r)(1-\\lambda-r)=0$\n",
    "\n",
    "Thus, eigenvalues for $\\Sigma$ are $\\lambda: (1+r$, $1-r$)\n",
    "\n",
    "\n",
    "- Given the condition that norm (i.e. the sum of squared values) of each eigenvector is equal to 1, what are the eigenvectors of co-variance matrix $\\Sigma$? ( For example, if an eigenvector is \n",
    "    ${v_1}=\\begin{bmatrix} \n",
    "    x1 \\\\ \n",
    "    x2 \n",
    "    \\end{bmatrix}$, then $x_1^2 + x_2^2 = 1$) [10pts]\n",
    "    \n",
    "Solving for eigenvector for the covariance matrix $\\Sigma$ for eigenvalue of $\\lambda = 1+r$\n",
    "\n",
    "$(A-\\lambda{I})X=0$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " 1-(1+r)&r\\\\\n",
    " r&1-(1+r)\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "                x_1\\\\\n",
    "                x_2\n",
    "                \\end{bmatrix}= 0$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " -r&r\\\\\n",
    " r&-r\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "                x_1\\\\\n",
    "                x_2\n",
    "                \\end{bmatrix}= 0$\n",
    " \n",
    "After gaussian elimination,\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-1&1\\\\\n",
    " 0&0\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "                x_1\\\\\n",
    "                x_2\n",
    "                \\end{bmatrix}= 0$\n",
    " \n",
    " $-x_1+x_2 = 0$\n",
    " \n",
    " $x_1=x_2$\n",
    " \n",
    " Applying given condition: norm of each eigenvector is 1\n",
    " \n",
    " $x_1^2+x_2^2=1$\n",
    " \n",
    " $2x_1^2=1$\n",
    " \n",
    " $x_1^2 = \\frac{1}{2}$\n",
    " \n",
    " Thus,\n",
    " \n",
    " $x_1 = x_2= \\frac{1}{\\sqrt{2}}$, or $-\\frac{1}{\\sqrt{2}}$\n",
    " \n",
    "**Eigenvector for eigenvalue of $1+r$ is $\\begin {bmatrix}\n",
    "                                               \\frac{1}{\\sqrt{2}}\\\\\n",
    "                                               \\frac{1}{\\sqrt{2}}\n",
    "                                               \\end{bmatrix}$ or $\\begin {bmatrix}\n",
    "                                                                   -\\frac{1}{\\sqrt{2}}\\\\\n",
    "                                                                   -\\frac{1}{\\sqrt{2}}\n",
    "                                                                    \\end{bmatrix}$ **\n",
    "\n",
    "\n",
    "Solving for eigenvector for the covariance matrix $\\Sigma$ for eigenvalue of $\\lambda = 1-r$\n",
    "\n",
    "$(A-\\lambda{I})X=0$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " 1-(1-r)&r\\\\\n",
    " r&1-(1-r)\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "                x_1\\\\\n",
    "                x_2\n",
    "                \\end{bmatrix}= 0$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " r&r\\\\\n",
    " r&r\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "                x_1\\\\\n",
    "                x_2\n",
    "                \\end{bmatrix}= 0$\n",
    " \n",
    "After gaussian elimination,\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " 1&1\\\\\n",
    " 0&0\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "                x_1\\\\\n",
    "                x_2\n",
    "                \\end{bmatrix}= 0$\n",
    " \n",
    " $x_1+x_2 = 0$\n",
    " \n",
    " $x_1=-x_2$\n",
    " \n",
    " Applying given condition: norm of each eigenvector is 1\n",
    " \n",
    " $x_1^2+x_2^2=1$\n",
    " \n",
    " $x_1^2+(-x_1)^2=1$\n",
    " \n",
    " $2x_1^2=1$\n",
    " \n",
    " $x_1^2 = \\frac{1}{2}$\n",
    " \n",
    " Thus,\n",
    " \n",
    " $x_1 = \\frac{1}{\\sqrt{2}}$, or $-\\frac{1}{\\sqrt{2}}$\n",
    "\n",
    " Since, $x_2=-x_1$\n",
    " \n",
    "**Eigenvector for eigenvalue of $1+r$ is $\\begin {bmatrix}\n",
    "                                               \\frac{1}{\\sqrt{2}}\\\\\n",
    "                                               -\\frac{1}{\\sqrt{2}}\n",
    "                                               \\end{bmatrix}$ or $\\begin {bmatrix}\n",
    "                                                                   -\\frac{1}{\\sqrt{2}}\\\\\n",
    "                                                                   \\frac{1}{\\sqrt{2}}\n",
    "                                                                    \\end{bmatrix}$ **\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Expectation, Co-variance and Independence [25pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X, Y$ and $Z$ are three different random variables.\n",
    "Let $X$ obeys Bernouli Distribution. The probability disbribution function is\n",
    "    $$p(x)=\\left\\{\n",
    "    \\begin{array}{c l}\t\n",
    "         0.5 & x = 1\\\\\n",
    "         0.5 & x = -1.\n",
    "    \\end{array}\\right.$$\n",
    "    \n",
    "Let $Y$ obeys the standard Normal (Gaussian) distribution, which can be written as $Y \\sim N(0,1)$. $X$ and $Y$ are independent. Meanwhile, let $Z = XY$.\n",
    "\n",
    "- What is the Expectation (mean value) of $X$? [3pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$E(X) = \\Sigma_{x\\epsilon(-1,1)}xP(x)$\n",
    "\n",
    "$E(x) = 1*P(1) + (-1)*P(-1)$\n",
    "\n",
    "$E(X) = 1*0.5+(-1)*(0.5)$\n",
    "\n",
    "Thus, $E(x) = 0.5 - 0.5 = 0$\n",
    "\n",
    "\n",
    "- Are Y and Z independent? (Just clarify, do not need to prove) [2pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "Since given that $Z=XY$, Value that z holds depends on values of X and Y. Implying that Y and Z are not independent.\n",
    "\n",
    "- Show that $Z$ is also a standard Normal (Gaussian) distribution, which means $Z \\sim N(0,1)$. [10pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "Using sum rule,\n",
    "\n",
    "$P(Z) = \\Sigma_{x\\epsilon(-1,1)}P(X,Z)$\n",
    "\n",
    "Applying product rule on right hand side of above eqn,\n",
    "\n",
    "$P(Z) = \\Sigma_{x\\epsilon(-1,1)}P(Z|X)P(X)$\n",
    "\n",
    "$P(Z) = \\Sigma_{x\\epsilon(-1,1)}P(XY|X)P(X)$\n",
    "\n",
    "$P(Z) = P(-Y)(0.5) + P(Y)(0.5)$\n",
    "\n",
    "Since, $Y$ obeys standard normal gaussian distribution, the value for $P(-Y) = P(Y)$\n",
    "\n",
    "Hence, $P(Z) = 2P(Y)(0.5) = P(Y)$\n",
    "\n",
    "Therefore, $Z$ also obeys standard normal gaussian distribution, which means $Z \\sim N(0,1)$\n",
    "\n",
    "\n",
    "- Are Y and Z uncorrelated(which means $Cov(Y,Z) = 0$)? (need to prove) [10pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$cov(Y,Z) = E[Y,Z]-E[Y]E[Z]$\n",
    "\n",
    "We proved above that, $P(Y) = P(Z)$\n",
    "\n",
    "Hence, $cov(Y,Z) = cov(Y,Y)$\n",
    "\n",
    "$cov(Y,Y) = E[Y^2]-E[Y][Y]$\n",
    "\n",
    "$E[Y^2]-E[Y][Y] = Var[Y] = 1$ (Standard gaussian normal distribution)\n",
    "\n",
    "Thus, since $cov(Y,Z) = 1 \\neq 0$, Y and Z are not uncorrelated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Maximum Likelihood [25pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Discrete Example [17pts]\n",
    "Suppose you are playing two unfair coins. The probability of tossing a head is $2 \\theta$ for coin 1, and $\\theta$ for coin 2. You toss each coin for several times, and you get the following results:\n",
    "\n",
    "| Coin No. | Result    |\n",
    "|------|------|\n",
    "|   1  | head |\n",
    "|   2  | head |\n",
    "|   1  | tail |\n",
    "|   2  | tail |\n",
    "|   1  | head |\n",
    "|   2  | tail |\n",
    "\n",
    "- What is the probability of tossing a tail for coin 1 ($p_{t1}$) and tossing a tail for coin 2 ($p_{t2}$)[3pts]? \n",
    "\n",
    "**Response**\n",
    "\n",
    "Based on the trial results, the probability of tossing a tail for coin 1 and coin 2 are:\n",
    "\n",
    "$p_{t1} = \\frac{1}{3}$\n",
    "\n",
    "$p_{t2} = \\frac{2}{3}$\n",
    "\n",
    "- What is the likelihood of the data given $\\theta$ [7pts]?\n",
    "\n",
    "**Response**\n",
    "\n",
    "Since each toss is independent event and each coin is flipped 3 times, the probability of obtaining 2 heads from 3 coin 1 flips and 1 head from 3 coin 2 flips is:\n",
    "\n",
    "$L(\\theta|x) = P(x|\\theta) =  {3 \\choose 2}(2\\theta)^2(1-2\\theta){3 \\choose 1}(\\theta)(1-\\theta)^2$\n",
    "\n",
    "$L(\\theta|x)= {3 \\choose 2}{3 \\choose 1} 4\\theta^2\\theta (1-2\\theta)(1-\\theta)^2$\n",
    "\n",
    "$L(\\theta|x)= {3 \\choose 2}{3 \\choose 1} 4\\theta^3(1-2\\theta)(1-\\theta)^2$\n",
    "\n",
    "\n",
    "- What is maximum likelihood estimation for $\\theta$ [7pts]?\n",
    "\n",
    "**Response**\n",
    "\n",
    "Finding maximum likelihood estimation for $\\theta$ means to find value of $\\theta$ that maximizes the likelihood function obtained above. That is, the value of $\\theta$ that maximizes likelihood of obtaining the trial results. \n",
    "\n",
    "Taking log of the likelihood function, we get,\n",
    "\n",
    "$log(L(\\theta|x))= log({3 \\choose 2}{3 \\choose 1}) +log(4\\theta^3)+ log(1-2\\theta)+log(1-\\theta)^2$\n",
    "\n",
    "$log(L(\\theta|x))= log({3 \\choose 2}{3 \\choose 1}) +3log(4\\theta)+ log(1-2\\theta)+2log(1-\\theta)$\n",
    "\n",
    "$\\frac{dL(\\theta|x)}{d\\theta} = \\frac{12}{4\\theta}+(\\frac{-2}{1-2\\theta})+(\\frac{-2}{1-\\theta})$\n",
    "\n",
    "Solving $\\frac{12}{4\\theta}+(\\frac{-2}{1-2\\theta})+(\\frac{-2}{1-\\theta}) = 0$, gives,\n",
    "\n",
    "$\\theta = \\frac{1}{3}  or  \\frac{3}{4}$\n",
    "\n",
    "Since given that the probability of obtaining head for coin 1 is $2\\theta$, value of $\\theta =\\frac{3}{4}$ is not possible.\n",
    "\n",
    "Hence, maximum likelihood estimation of $\\theta = \\frac{1}{3}$.\n",
    "\n",
    "### 3.2 Continues Example [8pts]\n",
    "\n",
    "A uniform distribution in the range of $[0, \\theta]$ is given by\n",
    "\n",
    "$$\n",
    "f(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{\\theta}} & {0 \\leq x \\leq \\theta} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "What is maximum likelihood estimation for $\\theta$?\n",
    "( **hint**: Think of two cases, where $\\theta < max(x_1, x_2, ..., x_n)$ and $\\theta \\ge max(x_1, x_2, ..., x_n).)$\n",
    "\n",
    "**Response**\n",
    "\n",
    "Likelihood function for the given uniform distribution is\n",
    "\n",
    "$L(\\theta|x_1,x_2,x_3,...x_n) = P(x_1,x_2,x_3,...x_n|\\theta) = P(x_1|\\theta).P(x_2|\\theta).P(x_3|\\theta)....P(x_n|\\theta)$\n",
    "\n",
    "$L(\\theta|x_1,x_2,x_3,...x_n) = \\frac{1}{\\theta^n} = \\theta^{-n}$\n",
    "\n",
    "$\\frac{d(L(\\theta|x_1,x_2,x_3,...x_n))}{d\\theta} = -n \\theta^{n-1}$\n",
    "\n",
    "If max of all the variables $(x_1,x_2, x_3,...x_n)$ is between $0$ and $\\theta$, then\n",
    "\n",
    "$L(\\theta|x_1,x_2,x_3,...x_n) =  \\theta^(-n)$, when $max(x_1,x_2,x_3,...x_n) \\leq \\theta$\n",
    "\n",
    "And otherwise $L(\\theta|x_1,x_2,x_3,...x_n) =  0$, when $max(x_1,x_2,x_3,...x_n) > \\theta$\n",
    "\n",
    "$\\frac{d(log(L(\\theta|x_1,x_2,x_3,...x_n)))}{\\theta} = \\frac{-n}{\\theta}$ for $max(x_1,x_2,x_3,...x_n) \\leq \\theta$\n",
    "\n",
    "Hence, $L(\\theta|x_1,x_2,x_3,...x_n)$ is a decreasing function, that is, function's value decreases with increasing value of $\\theta$. Also, considering the function holds for condition: $\\theta \\geq max(x_1, x_2, x_3,...x_n)$. Thus, the maximum value of function is obtained at minimum possible value of $\\theta$ which is $\\theta = max(x_1, x_2, x_3,...x_n)$.\n",
    "\n",
    "Thus, maximum value for likelihood will be obtained at $\\theta = max(x_1, x_2, x_3,...x_n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Information Theory [25pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Suppose the joint probability distribution of two binary random variables $X$ and $Y$ are given as follows.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\\hline X | Y & {1} & {2} \\\\ \\hline 0 & {\\frac{1}{4}} & {\\frac{1}{4}} \\\\ \\hline 1 & {\\frac{1}{2}} & {0} \\\\ \\hline\\end{array}\n",
    "$$\n",
    "\n",
    "- Show the marginal distribution of $X$. [2pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "Marginal distrbution of $X$ is given by:\n",
    "\n",
    "$p(x) = \\sum\\limits_yp(x,y)$\n",
    "\n",
    "$p(x=0) = \\sum\\limits_yp(x=0,y)$\n",
    "\n",
    "$p(x=0) = p(x=0,y=1)+p(x=0,y=2)$\n",
    "\n",
    "$p(x=0) = \\frac{1}{4}+\\frac{1}{4} = \\frac{1}{2}$\n",
    "\n",
    "$p(x=1) = \\sum\\limits_yp(x=1,y)$\n",
    "\n",
    "$p(x=1) = p(x=1,y=1)+p(x=1,y=2)$\n",
    "\n",
    "$p(x=1) = \\frac{1}{2}+0 = \\frac{1}{2}$\n",
    "\n",
    "Thus, marginal distribution of $X$: $p(x=0)= \\frac{1}{2}, p(x=1)= \\frac{1}{2}$\n",
    "\n",
    "- Find entropy $H(Y)$. [2pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$H(Y) = -\\sum\\limits_{k=1}^kP(y=k)logP(y=k)$\n",
    "\n",
    "$H(Y) = -P(y=1)logP(y=1)-P(y=2)logP(y=2)$\n",
    "\n",
    "Marginal distibution of $Y$ is:\n",
    "$P(y) = \\sum\\limits_xp(x,y)$\n",
    "\n",
    "$P(y=1) = p(x=0,y=1)+p(x=1,y=1) = \\frac{1}{4}+\\frac{1}{2} = \\frac{3}{4}$\n",
    "\n",
    "$P(y=2) = p(x=0,y=2)+p(x=1,y=2) = \\frac{1}{4} + 0 =\\frac{1}{4}$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$H(Y) = P(y=1)logP(y=1)+P(y=2)logP(y=2) = -\\frac{3}{4}.log\\frac{3}{4} - \\frac{1}{4}.log\\frac{1}{4}$\n",
    "\n",
    "$H(Y) = 0.311+0.5 = 0.811$\n",
    "\n",
    "- Find conditional entropy $H(X|Y)$ and $H(Y|X)$. [3pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "Average conditional entropy $H(X|Y) = \\sum\\limits_{y\\epsilon{1,2}}p(Y=y)H(X|Y=y)= P(Y=1)H(X|Y=1)+P(Y=2)H(X|Y=2)$\n",
    " \n",
    "$P(Y=1)H(X|Y=1) = \\frac{3}{4}H(\\frac{1}{3},\\frac{2}{3}) = 0.75(0.918) = 0.688$\n",
    "\n",
    "$P(Y=2)H(X|Y=2) = \\frac{1}{4}H(0,1)=0.25(H(0)+H(1))= 0*infinity+ 0 = undefined+0$\n",
    "\n",
    "Since H(X|Y=2) is sum of 2 components of certain probabilities of 0 and 1, there is no entropy.\n",
    "\n",
    "Thus, $H(X|Y) = H(X|Y=1)+H(X|Y=2) = 0.688$\n",
    "\n",
    "Average conditional entropy $H(Y|X) = \\sum\\limits_{x\\epsilon{0,1}}p(X=x)H(Y|X=x)= P(X=0)H(Y|X=0)+P(X=1)H(Y|X=1)$\n",
    " \n",
    "$P(X=0)H(Y|X=0) = \\frac{1}{2}H(\\frac{1}{2},\\frac{1}{2}) = 0.5(0.5+0.5)=0.5$\n",
    "\n",
    "$P(X=1)H(Y|X=1) = \\frac{1}{2}H(1,0)$\n",
    "Since H(Y|X=1) is sum of 2 components of certain probabilities of 0 and 1, there is no entropy.\n",
    "\n",
    "Thus, $H(Y|X) = 0.5$\n",
    "\n",
    "- Find mutual information $I(X;Y)$. [3pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$I(X;Y) = H(Y)-H(Y|X) = 0.811-0.5 = 0.311$\n",
    "\n",
    "- Find joint entropy $H(X, Y)$. [3pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$H(X,Y) = \\sum_{x,y}P(X=x,Y=y).log\\frac{1}{P(X=x,Y=y} = H(0.25,0.25,0.5,0) = 1.5$\n",
    "Since (Y=2,X=1) has  certain probability of 0, there is no entropy.\n",
    "\n",
    "**Note:** The following three proofs are not related to the example in the above questions. You need to prove each for any general case.\n",
    "- Suppose $X$ and $Y$ are independent. Show that $H(X|Y) = H(X)$. [4pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$H(X|Y) = \\sum_{x\\epsilon{X}}P(X=x).H(Y|X=x)$\n",
    "\n",
    "$H(X|Y) = \\sum_{x\\epsilon{X}}P(X=x).H(Y|X=x) = -\\sum_{x\\epsilon{X}}P(X=x|Y=y).logP(\\frac{X=x}{Y=y})$\n",
    "\n",
    "Because $X$ and $Y$ are independent. The above expression reduces to:\n",
    "\n",
    "$H(X|Y) = -\\sum_{x\\epsilon{X}}P(X=x).logP(X=x) = H(X)$\n",
    "\n",
    "Thus, $H(X|Y) = H(X)$\n",
    "\n",
    "- Suppose $X$ and $Y$ are independent. Show that $H(X,Y) = H(X) + H(Y)$. [4pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$H(X,Y) = -\\sum_{x,y\\epsilon{X,Y}}P(X=x,Y=y).logP(X=x,Y=y)$ \n",
    "\n",
    "$H(X,Y) = -\\sum_{x,y\\epsilon{X,Y}}P(X=x)P(Y=y)(logP(X=x)+logP(Y=y))$\n",
    "\n",
    "$H(X,Y) = -\\sum_{x\\epsilon{X}}\\sum_{y\\epsilon{Y}}P(X=x)P(Y=y)logP(X=x)+P(X=x)P(Y=y)logP(Y=y)$\n",
    "\n",
    "$H(X,Y) = -\\sum_{x\\epsilon{X}}\\sum_{y\\epsilon{Y}}P(X=x)P(Y=y)logP(X=x) -\\sum_{x\\epsilon{X}}\\sum_{y\\epsilon{Y}}P(X=x)P(Y=y)logP(Y=y)$\n",
    "\n",
    "$H(X,Y) = -\\sum_{x\\epsilon{X}}P(X=x)logP(X=x)\\sum_{y\\epsilon{Y}}P(Y=y)-\\sum_{x\\epsilon{X}}P(X=x)\\sum_{y\\epsilon{Y}}P(Y=y)logP(Y=y)$\n",
    "\n",
    "Since, $\\sum_{y\\epsilon{Y}}P(Y=y)=1$ and $\\sum_{x\\epsilon{X}}P(X=x)=1$,\n",
    "\n",
    "$H(X,Y) = -\\sum_{x\\epsilon{X}}P(X=x)logP(X=x)-\\sum_{y\\epsilon{Y}}P(Y=y)logP(Y=y)$\n",
    "\n",
    "Hence, $H(X,Y) = H(X)+H(Y)$\n",
    "\n",
    "- Show that $I(X;X) = H(X)$. [4pts]\n",
    "\n",
    "**Response**\n",
    "\n",
    "$I(X;X) = H(X) - H(X|X) = H(X) - (-\\sum_\\limits{x,x}P(x,x)log(P(x|x))$\n",
    "\n",
    "Since, $P(x|x) = 1$, probability of x to be equal to a value given it's value, is 1\n",
    "\n",
    "And since, $log 1 = 0$, the above expression reduces.\n",
    "\n",
    "$I(X;X) = H(X)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
